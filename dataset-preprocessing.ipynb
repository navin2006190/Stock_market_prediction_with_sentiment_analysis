{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12647148,"sourceType":"datasetVersion","datasetId":7992469},{"sourceId":12647150,"sourceType":"datasetVersion","datasetId":7992470},{"sourceId":12647182,"sourceType":"datasetVersion","datasetId":7992490}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install torch\n!pip install pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T06:18:39.345387Z","iopub.execute_input":"2025-08-02T06:18:39.345662Z","iopub.status.idle":"2025-08-02T06:20:07.481465Z","shell.execute_reply.started":"2025-08-02T06:18:39.345632Z","shell.execute_reply":"2025-08-02T06:20:07.480484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Install TextBlob\n!pip install textblob\n\n# Step 2: Import libraries\nimport pandas as pd\nimport re\nfrom textblob import TextBlob\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Step 3: Read raw content of the file\n# Since the CSV is malformed (no clean separation), we read it as text\nwith open('/kaggle/input/radditnews/RedditNews.csv', 'r', encoding='utf-8') as f:\n    lines = f.readlines()\n\nprint(\"First few lines of raw data:\")\nprint(\"\".join(lines[:2]) + \"\\n\")\n\n# Step 4: Parse lines and extract (date, headline) pairs\ndata = []\ndate_pattern = r'\\d{4}-\\d{2}-\\d{2}'  # Match YYYY-MM-DD\ncurrent_date = None\n\nfor line in lines:\n    # Split line into tokens (potential dates and headlines)\n    tokens = re.split(r'(?=\\d{4}-\\d{2}-\\d{2})', line.strip())\n\n    for token in tokens:\n        if not token:\n            continue\n\n        # Extract date at the beginning of token\n        match = re.match(date_pattern, token)\n        if match:\n            current_date = match.group()\n            # The rest is the headline\n            headline = token[len(current_date):].strip()\n        else:\n            headline = token.strip()\n\n        # Skip if no valid date or empty headline\n        if not current_date or not headline:\n            continue\n\n        data.append({'Date': current_date, 'Headline': headline})\n\n# Step 5: Create DataFrame\ndf = pd.DataFrame(data)\nprint(f\"\\n Extracted {len(df)} headlines from {df['Date'].nunique()} unique days.\")\n\n# Step 6: Clean Headline column\ndef clean_headline(text):\n    # Remove b'...' or b\"...\" wrapper\n    text = re.sub(r\"^b'(.*)'$\", r\"\\1\", text)\n    text = re.sub(r'^b\"(.*)\"$', r'\\1', text)\n    # Remove escaped quotes and backslashes\n    text = text.replace(\"\\\\'\", \"'\").replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n    text = text.replace(\"\\\\r\", \" \").replace(\"\\\\n\", \" \").strip()\n    return text\n\ndf['Headline'] = df['Headline'].astype(str).apply(clean_headline)\n\n# Filter out placeholder-like or garbage\ninvalid = ['nan', '', 'b?', 'b', '\"\"', \"''\"]\ndf = df[~df['Headline'].isin(invalid)]\ndf = df[df['Headline'].str.len() > 5]\n\n# Step 7: Sentiment analysis function\ndef get_sentiment(text):\n    return TextBlob(text).sentiment.polarity  # Returns -1 to 1\n\nprint(\"\\n Computing sentiment for each headline...\")\ndf['Sentiment'] = df['Headline'].apply(get_sentiment)\n\n# Step 8: Group by Date â†’ average sentiment\ndaily_sentiment = df.groupby('Date')['Sentiment'].mean().reset_index()\ndaily_sentiment.rename(columns={'Sentiment': 'Daily_Sentiment'}, inplace=True)\ndaily_sentiment['Date'] = pd.to_datetime(daily_sentiment['Date'])  # Ensure proper date type\n\n# Sort by date\ndaily_sentiment.sort_values('Date', inplace=True)\ndaily_sentiment.reset_index(drop=True, inplace=True)\n\n# Step 9: Display results\nprint(\"\\n Sample Daily Sentiment (first 10 days):\")\nprint(daily_sentiment.head(10))\n\nprint(\"\\n Daily Sentiment Statistics:\")\nprint(daily_sentiment['Daily_Sentiment'].describe())\n\n# Step 10: Save and download\noutput_file = 'RedditNews_Daily_Sentiment.csv'\ndaily_sentiment.to_csv(output_file, index=False, encoding='utf-8')\nprint(f\"\\n Saved daily sentiment to: {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T06:20:10.754667Z","iopub.execute_input":"2025-08-02T06:20:10.755569Z","iopub.status.idle":"2025-08-02T06:20:28.915224Z","shell.execute_reply.started":"2025-08-02T06:20:10.755538Z","shell.execute_reply":"2025-08-02T06:20:28.914144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# To get sentiment score:","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\ndef get_sentiment_score(sentence):\n    # Create a TextBlob object\n    blob = TextBlob(sentence)\n\n    # Get the polarity score (-1 to 1)\n    sentiment_score = blob.sentiment.polarity\n\n    return sentiment_score\n\n# Get user input\nuser_sentence = input(\"Enter a sentence to analyze its sentiment: \")\n\n# Calculate and display the sentiment score\nscore = get_sentiment_score(user_sentence)\nprint(f\"\\nSentiment score for '{user_sentence}': {score:.3f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T06:20:37.253189Z","iopub.execute_input":"2025-08-02T06:20:37.254167Z","iopub.status.idle":"2025-08-02T06:23:57.898666Z","shell.execute_reply.started":"2025-08-02T06:20:37.254126Z","shell.execute_reply":"2025-08-02T06:23:57.898099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport yfinance as yf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T06:24:02.614088Z","iopub.execute_input":"2025-08-02T06:24:02.614779Z","iopub.status.idle":"2025-08-02T06:24:03.477502Z","shell.execute_reply.started":"2025-08-02T06:24:02.614755Z","shell.execute_reply":"2025-08-02T06:24:03.476950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start = '2008-06-08'\nend ='2016-07-01'\n\nstock ='GOOG'\ndata = yf.download(stock, start=start, end=end)\n\n# Save to CSV\ndata.to_csv('GOOG_2008-2016.csv')\nprint(\"Data downloaded and saved to GOOG_2008-2016.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T06:24:07.990011Z","iopub.execute_input":"2025-08-02T06:24:07.990457Z","iopub.status.idle":"2025-08-02T06:24:09.509079Z","shell.execute_reply.started":"2025-08-02T06:24:07.990433Z","shell.execute_reply":"2025-08-02T06:24:09.508315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the datasets\ntry:\n    df_reddit = pd.read_csv('RedditNews_Daily_Sentiment.csv')\n    df_goog = pd.read_csv('GOOG_2008-2016.csv')\nexcept FileNotFoundError as e:\n    print(f\"Error loading file: {e}. Please ensure the files are uploaded to your Colab environment.\")\n    exit()\n\n# Preprocess GOOG data\n# Drop the first two rows (redundant headers)\ndf_goog = df_goog.iloc[2:].copy()\n\n# Rename the first column to 'Date' for merging\ndf_goog.rename(columns={df_goog.columns[0]: 'Date'}, inplace=True)\n\n# Convert 'Date' columns to datetime objects for accurate merging\ndf_reddit['Date'] = pd.to_datetime(df_reddit['Date'])\ndf_goog['Date'] = pd.to_datetime(df_goog['Date'])\n\n# Merge the two DataFrames based on the 'Date' column\nmerged_df = pd.merge(df_goog, df_reddit, on='Date', how='left')\n\n# Handle missing values in 'Daily_Sentiment' column\n# It's good practice to inspect how many missing values are there before filling\nprint(\"Number of missing values in 'Daily_Sentiment' before handling:\")\nprint(merged_df['Daily_Sentiment'].isnull().sum())\n\n# For simplicity, we'll fill missing sentiment values with 0,\n# assuming no sentiment data implies a neutral sentiment.\n# You might choose a different strategy based on your analysis needs (e.g., forward fill, mean)\nmerged_df['Daily_Sentiment'].fillna(0, inplace=True)\n\nprint(\"\\nNumber of missing values in 'Daily_Sentiment' after handling:\")\nprint(merged_df['Daily_Sentiment'].isnull().sum())\n\n# Display the first few rows of the merged DataFrame\nprint(\"\\nFirst 5 rows of the merged DataFrame:\")\nprint(merged_df.head())\n\n# Display information about the merged DataFrame to confirm data types and non-null counts\nprint(\"\\nInformation about the merged DataFrame:\")\nprint(merged_df.info())\n\n# Optionally, save the merged DataFrame to a new CSV file\nmerged_df.to_csv('GOOG_with_Sentiment.csv', index=False)\nprint(\"\\nMerged DataFrame saved to 'GOOG_with_Sentiment.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T06:24:12.240841Z","iopub.execute_input":"2025-08-02T06:24:12.241139Z","iopub.status.idle":"2025-08-02T06:24:12.290822Z","shell.execute_reply.started":"2025-08-02T06:24:12.241117Z","shell.execute_reply":"2025-08-02T06:24:12.290094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# --- Step 1: Load the datasets ---\ntry:\n    df_reddit = pd.read_csv('RedditNews_Daily_Sentiment.csv')\n    df_goog = pd.read_csv('GOOG_2008-2016.csv')\n    print(\"Datasets loaded successfully.\")\nexcept FileNotFoundError as e:\n    print(f\"Error loading file: {e}. Please ensure the files are uploaded to your Colab environment.\")\n    exit()\n\n# --- Step 2: Preprocess GOOG Data ---\n# Drop the first two rows (redundant headers) from GOOG data\ndf_goog = df_goog.iloc[2:].copy()\nprint(\"Dropped redundant header rows from GOOG data.\")\n\n# Rename the first column to 'Date' for merging\ndf_goog.rename(columns={df_goog.columns[0]: 'Date'}, inplace=True)\nprint(\"Renamed first column of GOOG data to 'Date'.\")\n\n# Convert 'Date' columns to datetime objects for accurate merging\ndf_reddit['Date'] = pd.to_datetime(df_reddit['Date'])\ndf_goog['Date'] = pd.to_datetime(df_goog['Date'])\nprint(\"Converted 'Date' columns to datetime format.\")\n\n# --- Step 3: Merge DataFrames ---\n# Merge GOOG data with Reddit sentiment data based on the 'Date' column\n# 'how='left'' ensures all GOOG dates are kept, filling NaN for unmatched sentiment dates\nmerged_df = pd.merge(df_goog, df_reddit, on='Date', how='left')\nprint(\"Merged GOOG data with Reddit sentiment data.\")\n\n# Handle missing values in 'Daily_Sentiment' column\n# Fill NaN values (where no sentiment data exists for a date) with 0, assuming neutral sentiment.\n# Consider other imputation strategies (e.g., forward fill, mean) if your analysis requires.\ninitial_nulls = merged_df['Daily_Sentiment'].isnull().sum()\nmerged_df['Daily_Sentiment'].fillna(0, inplace=True)\nprint(f\"Filled {initial_nulls} missing sentiment values with 0.\")\n\n# --- Step 4: Add 'tomorrow's closing value' as the output feature ---\n# Create the 'tomorrow's closing value' column by shifting the 'Close' column upwards by 1\n# This makes the closing value of the next day the target for the current day's features.\nmerged_df['tomorrow\\'s closing value'] = merged_df['Close'].shift(-1)\nprint(\"Added 'tomorrow\\'s closing value' column by shifting 'Close' prices.\")\n\n# Handle the last cell of the 'tomorrow's closing value' column\n# The last value will be NaN after the shift. As requested, we fill it\n# with the 'Close' value from the second-to-last day (previous 2 value).\nif not merged_df.empty and pd.isna(merged_df.iloc[-1]['tomorrow\\'s closing value']):\n    if len(merged_df) >= 2:\n        # Fill the last row's 'tomorrow\\'s closing value' with the 'Close' of the second to last day\n        merged_df.loc[merged_df.index[-1], 'tomorrow\\'s closing value'] = merged_df.iloc[-2]['Close']\n        print(\"Filled the last 'tomorrow\\'s closing value' with the 'Close' price from the second to last day.\")\n    elif len(merged_df) == 1:\n        # Edge case: if only one row, fill with its own close value.\n        merged_df.loc[merged_df.index[-1], 'tomorrow\\'s closing value'] = merged_df.iloc[-1]['Close']\n        print(\"Dataset has only one row; filled 'tomorrow\\'s closing value' with current day's 'Close'.\")\n\n# --- Step 5: Display and Save Results ---\nprint(\"\\n--- Processed DataFrame Overview ---\")\nprint(\"First 5 rows:\")\nprint(merged_df.head())\n\nprint(\"\\nLast 5 rows:\")\nprint(merged_df.tail())\n\nprint(\"\\nDataFrame Information (dtypes and non-null counts):\")\nprint(merged_df.info())\n\n# Optionally, save the final processed DataFrame to a new CSV file\noutput_filename = 'GOOG_with_Tomorrow_Close_Final.csv'\nmerged_df.to_csv(output_filename, index=False)\nprint(f\"\\nFinal processed DataFrame saved to '{output_filename}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T06:24:17.121290Z","iopub.execute_input":"2025-08-02T06:24:17.121994Z","iopub.status.idle":"2025-08-02T06:24:17.170975Z","shell.execute_reply.started":"2025-08-02T06:24:17.121967Z","shell.execute_reply":"2025-08-02T06:24:17.170153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**After preprocessing we find the final dataset neamed GOOG_with_Tomorrow_Close_Final.csv**","metadata":{}}]}